{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Personality Analysis - Unsupervised Learning Project\n\nKaggle Dataset: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis\n        \nKaggle Notebook: https://www.kaggle.com/code/robinlutter/customer-personality-analysis-pca-and-clustering\n\nGitHub: https://github.com/wr0b1n/MSDS-5510-FinalProject","metadata":{"execution":{"iopub.status.busy":"2023-07-20T18:01:56.013746Z","iopub.execute_input":"2023-07-20T18:01:56.014154Z","iopub.status.idle":"2023-07-20T18:01:56.022465Z","shell.execute_reply.started":"2023-07-20T18:01:56.014122Z","shell.execute_reply":"2023-07-20T18:01:56.020798Z"}}},{"cell_type":"markdown","source":"# 1. Project Topic and Goal\n\nIn this project we want to build an unsupervised machine learning model that is able to perform a clustering for a  segmentation task on customer data. To achieve this, we are working with a public dataset from Kaggle that is not part of a competition which means there is no pre-specified evaluation metric. The goal is to compare different clustering algorithms with each other and see how well they perform. Besides typical clustering methods like K-Means we may need to perform a PCA to reduce the amount of dimensions in our dataset. \n\nWhy would we even want to perform such a customer segmentation from a business point of view?\n\nThe reason is that this analysis empowers businesses to make the right product modifications or marketing strategies that align precisely with the preferences of specific customer segments. For example, rather than investing resources in marketing a new product to every customer available, the company can identify the most receptive customer segment and focus its marketing efforts only on that particular group. This targeted approach enhances efficiency, saves costs, and increases the likelihood of successful product adoption. This means we can understand our customer's needs better, distinct them by their buying behavior and ultimately adapt our marketing strategy and product development accordingly.\n\nDuring this project we aim for answering the following business question. Can we segment our customers by looking at their income and money spent on our products? This will hopefully help us to target appropriate marketing strategies at certain groups of customers and finally increase our revenue. This way we won't waste effort in marketing products on people that won't buy them anyway but instead make our marketing strategy much more effective.","metadata":{}},{"cell_type":"markdown","source":"# 2. Data\n\nThe project is based on the following public Kaggle dataset: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis\n\nThe dataset itself has a CSV format with a size of around 220kB which is really small. This means we don't have to worry processing large amounts of data. The format is tabular with 29 columns and 2240 rows in total.\n\nA first glance at the data shows that we have to work with features both numerical and categorical. We will inspect this in more detail during the EDA part. The features itself can be grouped into different categories:\n\n* People (Year of birth, education level, ...)\n* Products (Amount spent on fruits in last 2 years, ...)\n* Promotion (Number of purchases made with discount, ...)\n* Place (Number of purchases made through website, ...)","metadata":{}},{"cell_type":"markdown","source":"# 3. Import Python Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import silhouette_score, make_scorer\nfrom sklearn.metrics import davies_bouldin_score\n\nimport warnings\nimport sys\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.159578Z","iopub.execute_input":"2023-07-29T09:55:40.160005Z","iopub.status.idle":"2023-07-29T09:55:40.169815Z","shell.execute_reply.started":"2023-07-29T09:55:40.159964Z","shell.execute_reply":"2023-07-29T09:55:40.168461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Exploratory Data Analysis\n\nWe will start with exploring our data set several built-in dataframe methods and visualizations.","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Data Description","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv', sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.176669Z","iopub.execute_input":"2023-07-29T09:55:40.177064Z","iopub.status.idle":"2023-07-29T09:55:40.218416Z","shell.execute_reply.started":"2023-07-29T09:55:40.177033Z","shell.execute_reply":"2023-07-29T09:55:40.217047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.220564Z","iopub.execute_input":"2023-07-29T09:55:40.220927Z","iopub.status.idle":"2023-07-29T09:55:40.249618Z","shell.execute_reply.started":"2023-07-29T09:55:40.220895Z","shell.execute_reply":"2023-07-29T09:55:40.248280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.251936Z","iopub.execute_input":"2023-07-29T09:55:40.252309Z","iopub.status.idle":"2023-07-29T09:55:40.271561Z","shell.execute_reply.started":"2023-07-29T09:55:40.252278Z","shell.execute_reply":"2023-07-29T09:55:40.270245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are working with a dataset of 2240 rows and 29 columns. We inspect that there are some missing values in the \"Income\" column. We will take care of these in the next part.\n\nBelow we have listed the categorical and numerical features which we will make use of later. ","metadata":{}},{"cell_type":"code","source":"categorical_features = ['Education', 'Marital_Status', 'Complain', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\nprint(categorical_features)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.274731Z","iopub.execute_input":"2023-07-29T09:55:40.275102Z","iopub.status.idle":"2023-07-29T09:55:40.283407Z","shell.execute_reply.started":"2023-07-29T09:55:40.275069Z","shell.execute_reply":"2023-07-29T09:55:40.281960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = [feature for feature in df.columns.tolist() if feature not in categorical_features]\nprint(numerical_features)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.285291Z","iopub.execute_input":"2023-07-29T09:55:40.285661Z","iopub.status.idle":"2023-07-29T09:55:40.300188Z","shell.execute_reply.started":"2023-07-29T09:55:40.285629Z","shell.execute_reply":"2023-07-29T09:55:40.298605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Data Cleaning and Preprocessing\n\n### 4.2.1 Missing Values\n\nFirst, we check for NA or NULL values using the built-in functions of pandas dataframes (to confirm our inspection of missing values from before).","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.301775Z","iopub.execute_input":"2023-07-29T09:55:40.302912Z","iopub.status.idle":"2023-07-29T09:55:40.325561Z","shell.execute_reply.started":"2023-07-29T09:55:40.302870Z","shell.execute_reply":"2023-07-29T09:55:40.324289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.326944Z","iopub.execute_input":"2023-07-29T09:55:40.327912Z","iopub.status.idle":"2023-07-29T09:55:40.342627Z","shell.execute_reply.started":"2023-07-29T09:55:40.327873Z","shell.execute_reply":"2023-07-29T09:55:40.341161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just in case, we can also search for empty (or just spaces) string values. ","metadata":{}},{"cell_type":"code","source":"np.where(df.applymap(lambda x: str(x).strip() == ''))","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.344820Z","iopub.execute_input":"2023-07-29T09:55:40.345707Z","iopub.status.idle":"2023-07-29T09:55:40.413452Z","shell.execute_reply.started":"2023-07-29T09:55:40.345653Z","shell.execute_reply":"2023-07-29T09:55:40.411751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The observation shows that there are indeed 24 NA respectively NULL values in the 'Income' column but no empty strings or spaces in any other column. Since income might be an important indicator wether a customer belongs to this or another cluster we should definitely take care of them. There are several commonly used strategies:\n\n* Impute missing values based on their underlying distribution\n* Drop rows with empty values\n* Use advanced imputation techniques like regression imputation\n\nWe will start by looking at the distribution of the valid values.","metadata":{}},{"cell_type":"code","source":"def plot_income_dist(df):\n    plt.hist(df['Income'], bins=50)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Income')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.415235Z","iopub.execute_input":"2023-07-29T09:55:40.415834Z","iopub.status.idle":"2023-07-29T09:55:40.428022Z","shell.execute_reply.started":"2023-07-29T09:55:40.415793Z","shell.execute_reply":"2023-07-29T09:55:40.427121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_income_dist(df)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.433214Z","iopub.execute_input":"2023-07-29T09:55:40.434189Z","iopub.status.idle":"2023-07-29T09:55:40.797985Z","shell.execute_reply.started":"2023-07-29T09:55:40.434136Z","shell.execute_reply":"2023-07-29T09:55:40.796766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histogram we can assume that most of the income data follows a normal distribution. But we have a few outliers to to the right side at an income of approximately 150000 and even one extreme outlier at an income of over 600000. Let's define a treshold at 200000 and see how many samples fall below and above it.  ","metadata":{}},{"cell_type":"code","source":"# calculate percentage of samples above and below the threshold\nthreshold = 200000\nabove_threshold = df[df['Income'] > threshold].shape[0]\nbelow_threshold = df[df['Income'] <= threshold].shape[0]\ntotal_samples = above_threshold + below_threshold\n\npercentage_above_threshold = (above_threshold / total_samples) * 100\npercentage_below_threshold = (below_threshold / total_samples) * 100\n\n# create pie chart\nlabels = ['Above Threshold', 'Below Threshold']\nsizes = [percentage_above_threshold, percentage_below_threshold]\ncolors = ['skyblue', 'lightgreen']\nexplode = (0.2, 0)\n\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.title(f'Pie chart for distribution of Income (Threshold = {threshold})')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.799388Z","iopub.execute_input":"2023-07-29T09:55:40.799722Z","iopub.status.idle":"2023-07-29T09:55:40.992563Z","shell.execute_reply.started":"2023-07-29T09:55:40.799692Z","shell.execute_reply":"2023-07-29T09:55:40.991315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since only around 0.05% of our customers have an income of over 200000 we can safely ignore these by dropping the respective samples. Since there are still a few customers with income of around 150000 we cannot assume normality for the distribution due to the skewness to the right side. But we do not want to drop them since they could be part of an high income cluster or even form its own cluster. As a consequence, we cannot simply impute the missing values by using the mean but rather use the median value. ","metadata":{}},{"cell_type":"code","source":"# drop customers with income greater than 200000\nmask = df['Income'] > 200000\ndf = df[~mask]","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:40.994522Z","iopub.execute_input":"2023-07-29T09:55:40.995053Z","iopub.status.idle":"2023-07-29T09:55:41.003956Z","shell.execute_reply.started":"2023-07-29T09:55:40.995011Z","shell.execute_reply":"2023-07-29T09:55:41.001979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute median income for missing values\nmedian_income = df['Income'].median()\ndf['Income'].fillna(median_income, inplace=True)\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:41.005915Z","iopub.execute_input":"2023-07-29T09:55:41.007137Z","iopub.status.idle":"2023-07-29T09:55:41.027434Z","shell.execute_reply.started":"2023-07-29T09:55:41.007089Z","shell.execute_reply":"2023-07-29T09:55:41.026199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output shows that there are no missing values anymore. Again, we can have a look at the histogram to see the underlying distribution in greater resolution containing the imputed values.","metadata":{}},{"cell_type":"code","source":"plot_income_dist(df)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:41.029458Z","iopub.execute_input":"2023-07-29T09:55:41.030650Z","iopub.status.idle":"2023-07-29T09:55:41.356957Z","shell.execute_reply.started":"2023-07-29T09:55:41.030603Z","shell.execute_reply":"2023-07-29T09:55:41.355875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3 Inspecting numerical features\n\nNext, we will inspect all the numerical features and check whether there are any other extreme outliers present in the data.","metadata":{}},{"cell_type":"code","source":"# make a grid for all the plots\nnum_plots = len(numerical_features)\nnum_cols = 4\nnum_rows = int(num_plots / num_cols)\n\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\nfig.suptitle('Numerical Features Plots', fontsize=16)\n\n# flattens the axs array (otherwise axs[i].hist does not work)\naxs = axs.ravel()\n\n# make a plot for each numerical feature\nfor i, column in enumerate(numerical_features):\n    axs[i].hist(df[column], bins=20)\n    axs[i].set_title(column)\n    axs[i].set_xlabel('Values')\n    axs[i].set_ylabel('Frequency')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:41.358265Z","iopub.execute_input":"2023-07-29T09:55:41.358579Z","iopub.status.idle":"2023-07-29T09:55:50.433803Z","shell.execute_reply.started":"2023-07-29T09:55:41.358552Z","shell.execute_reply":"2023-07-29T09:55:50.432542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing to notice here is that both the 'Z_CostContact' and 'Z_Revenue' column have the same value over all samples. Unfortunetaly, the dataset provides no explanation what these two columns are about. But since there is always the same value there is no impact on our model building and we can simply drop them.\n\nWe can also see that the ID column (by nature) has no significant meaning for our clustering approach. We will drop this one as well.\n\nMany features seem to follow some sort of exponential distribution. For example, looking at the 'MntFishProducts' column that means that most of our customers have spent a pretty small amount of money on fish in last 2 years. This makes sense since fish often times can be quite expensive and less people have enough buying power to purchase large amounts of it.\n\nThere are also some features with outliers. For example, the 'NumDealsPurchases' column which represent the number of purchases made with a discount has quite a few samples with higher numbers. But again, this could be of significant meaning to our clutering goal and potentially form its own cluster. Therefore we won't drop those samples.","metadata":{}},{"cell_type":"code","source":"# drop irrelevant columns\ncolumns_to_drop = ['ID', 'Z_CostContact', 'Z_Revenue']\ndf.drop(columns=columns_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:50.435303Z","iopub.execute_input":"2023-07-29T09:55:50.435665Z","iopub.status.idle":"2023-07-29T09:55:50.443673Z","shell.execute_reply.started":"2023-07-29T09:55:50.435634Z","shell.execute_reply":"2023-07-29T09:55:50.442281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should also take care of the \"Dt_Customer\" column which represent the date of the customer's enrollment with the company. This representation will likely cause problems when applying techniques as PCA and model building due to its datatype. Therefore, we will convert this column into a new column that represents the number of days the customer is enrolled with the company which we can represent using an integer.","metadata":{}},{"cell_type":"code","source":"# convert to datetime format\ndf['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\ncurrent_date = specified_datetime = datetime.strptime(\"29-07-2023\", \"%d-%m-%Y\")\n\n# get number of days since enrollment\ndf['DaysEnrolled'] = (current_date - df['Dt_Customer']).dt.days\n\n# drop original column since it is not needed anymore\ndf.drop('Dt_Customer', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:50.445458Z","iopub.execute_input":"2023-07-29T09:55:50.446301Z","iopub.status.idle":"2023-07-29T09:55:50.474905Z","shell.execute_reply.started":"2023-07-29T09:55:50.446266Z","shell.execute_reply":"2023-07-29T09:55:50.473622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The fact that we use the current date as a reference should not impact model performance since we will standardize our data before applying PCA and model building. The main focus here is to catch the relative durations for each customer to compare them with each other.\n\nSince clustering methods can suffer from high dimensionality we should aim for reducing dimensions as much as possible. For this purpose we will combine all columns related to spending a certain amount on a specific product and create a new column that shows the total amount spent.","metadata":{}},{"cell_type":"code","source":"# combine columns and drop original ones\ndf['TotalSpent'] = df['MntWines'] + df['MntFruits'] + df['MntMeatProducts'] + df['MntFishProducts'] + df['MntSweetProducts'] + df['MntGoldProds']\ncolumns_to_drop = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\ndf.drop(columns=columns_to_drop, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:50.476321Z","iopub.execute_input":"2023-07-29T09:55:50.476652Z","iopub.status.idle":"2023-07-29T09:55:50.487594Z","shell.execute_reply.started":"2023-07-29T09:55:50.476622Z","shell.execute_reply":"2023-07-29T09:55:50.486386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:50.489159Z","iopub.execute_input":"2023-07-29T09:55:50.489635Z","iopub.status.idle":"2023-07-29T09:55:50.518116Z","shell.execute_reply.started":"2023-07-29T09:55:50.489604Z","shell.execute_reply":"2023-07-29T09:55:50.516790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.4 Inspecting categorical features\n\nIn this step, we will have a look at the categorical features by creating barplots using seaborn. We will see how the different categories are encoded and if we have to clean things up.","metadata":{}},{"cell_type":"code","source":"# make a grid for the plots\nnum_plots = len(categorical_features)\nnum_cols = 2\nnum_rows = (num_plots - 1) // num_cols + 1\n\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\nfig.suptitle('Categorical Features Plots', fontsize=16)\n\n# flattens the axs array (otherwise axs[i].bar does not work)\naxs = axs.ravel()\n\n# plot all categorical features\nfor i, column in enumerate(categorical_features):\n    counts = df[column].value_counts()\n    axs[i].bar(counts.index, counts.values)\n    axs[i].set_title(column)\n    axs[i].set_xlabel('Categories')\n    axs[i].set_ylabel('Count')\n    \n# hide unused grid space\nfor i in range(len(categorical_features), num_rows * num_cols):\n    axs[i].axis('off')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:50.520024Z","iopub.execute_input":"2023-07-29T09:55:50.520564Z","iopub.status.idle":"2023-07-29T09:55:52.509692Z","shell.execute_reply.started":"2023-07-29T09:55:50.520519Z","shell.execute_reply":"2023-07-29T09:55:52.508366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Except for 'Education' and 'Marital_Status' all categorical features take on only 2 possible values 0 and 1.\n\nMost of our customers are married or at least living together. For the 'Marital_Status' feature we have many different possible values. There are also some categories like 'YOLO' or 'Absurd' that don't make sense and are not explained any futher in the dataset description on Kaggle. We will drop these two since they only represent a very small part of the samples. To reduce model complexity futher we will combine 'Married' and 'Together' to create a new category 'Relationship' and we will also combine 'Single', 'Divorced', 'Widow', and 'Alone' to a new category 'No Relationship'.\n\nFor 'Education' it is quite hard to say what 'Graduation', '2n Cycle', and 'Basic' means. I could not find any explanation on Kaggle for this. Since those categories are not negligible due to the corresponding amount of customers we will leave them as they are. ","metadata":{}},{"cell_type":"code","source":"# remove samples with 'Absurd' and 'YOLO' marital status\nmask1 = df['Marital_Status'] == 'Absurd'\nmask2 = df['Marital_Status'] == 'YOLO'\ndf = df[~mask1]\ndf = df[~mask2]\n\n# combine categories of marital status\ncategory_mapping = {\n    'Married': 'Relationship',\n    'Together': 'Relationship',\n    'Single': 'No Relationship',\n    'Widow': 'No Relationship',\n    'Alone': 'No Relationship',\n    'Divorced': 'No Relationship',\n}\n\ndf['Marital_Status'] = df['Marital_Status'].replace(category_mapping)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:52.511378Z","iopub.execute_input":"2023-07-29T09:55:52.512347Z","iopub.status.idle":"2023-07-29T09:55:52.530310Z","shell.execute_reply.started":"2023-07-29T09:55:52.512302Z","shell.execute_reply":"2023-07-29T09:55:52.529191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a barplot of the combined marital status feature\ncolumn_to_plot = 'Marital_Status'\ncategory_counts = df[column_to_plot].value_counts()\nsorted_categories = category_counts.sort_values(ascending=False).index\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x=column_to_plot, data=df, order=sorted_categories)\nplt.title(f'Barplot of {column_to_plot}', fontsize=16)\nplt.xlabel(column_to_plot)\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:52.531853Z","iopub.execute_input":"2023-07-29T09:55:52.532659Z","iopub.status.idle":"2023-07-29T09:55:52.797685Z","shell.execute_reply.started":"2023-07-29T09:55:52.532624Z","shell.execute_reply":"2023-07-29T09:55:52.796358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to reduce the number of dimensions further, we could combine the \"AcceptedCmp...\" and \"Response\" columns by creating a new column that represents whether a customer is susceptible for marketing campaigns or not.","metadata":{}},{"cell_type":"code","source":"# create combined column and drop original ones\ncampaign_columns = ['AcceptedCmp1','AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\ndf['CampaignAccepted'] = df[campaign_columns].any(axis=1)\ndf = df.drop(campaign_columns, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:52.799461Z","iopub.execute_input":"2023-07-29T09:55:52.799994Z","iopub.status.idle":"2023-07-29T09:55:52.811310Z","shell.execute_reply.started":"2023-07-29T09:55:52.799948Z","shell.execute_reply":"2023-07-29T09:55:52.809992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:52.813333Z","iopub.execute_input":"2023-07-29T09:55:52.813793Z","iopub.status.idle":"2023-07-29T09:55:52.835897Z","shell.execute_reply.started":"2023-07-29T09:55:52.813753Z","shell.execute_reply":"2023-07-29T09:55:52.834467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After all these steps we have reduced our dataset down to only 16 columns.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.4 One-Hot Encoding\n\nNow that we have inspected our categorical variables we need to preprocess them in order to use them in our machine learning models. One popular method is called One-hot encoding and is a technique used in machine learning to convert categorical data into a numerical format. Each category in the original feature is transformed into a new binary feature, where 1 represents the presence of the category and 0 represents the absence. This way we preserve all the information in our dataset by making each category a seperate feature.","metadata":{}},{"cell_type":"code","source":"# apply one-hot encoding to our categorical columns\ndf_encoded = pd.get_dummies(df, columns=['Education', 'Marital_Status', 'CampaignAccepted'])\ndf_encoded","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:52.848761Z","iopub.execute_input":"2023-07-29T09:55:52.849296Z","iopub.status.idle":"2023-07-29T09:55:52.888856Z","shell.execute_reply.started":"2023-07-29T09:55:52.849256Z","shell.execute_reply":"2023-07-29T09:55:52.887499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One major disadvantage of this technique is the increasing number of dimensions in our dataset. After the encoding step we now have 22 features. The encoding results in a more sparse dataset, which might negatively impact the performance of some algorithms and increase computational requirements.\n\nThinking of clustering this means we get very high dimensional sparse spaces where our algorithm has to find points that are close together. Espacially with distance based algorithms like K-Means this can cause severe problems. Also, the features in high dimensions tend to be redundant and correlated with each other which is why models are likely to overfit as a consequence. For example, K-Means clustering is very vulnerable to this curse of dimensionality. One common approach to take care of this is to perform a Principal Component Analysis (PCA) upfront.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.5 PCA\n\nPrincipal Component Analysis is a commonly used technique for dimensionality reduction on large datasets. It allows us to transform a dataset with a large number of features into a lower-dimensional space while still preserving the most important patterns by finding the components that maximize the explained variance. Therefore, the resulting set of transformed features are called principal components. They are uncorrelated by nature since principal components are orthogonal to each other and ranked by their explained variance.","metadata":{}},{"cell_type":"code","source":"n_components_range = range(len(df_encoded.columns))\n\n# standardize data before PCA\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_encoded)\n\n# PCA for each number of components\nexplained_variance = []\ncumulative_variance = []\nfor n_components in n_components_range:\n    pca = PCA(n_components = n_components)\n    pca.fit(scaled_data)\n    explained_variance.append(np.sum(pca.explained_variance_ratio_))\n    cumulative_variance.append(np.sum(pca.explained_variance_ratio_))\n    \n# plot the cumulative explained variance\nplt.figure(figsize=(10, 6))\nplt.plot(n_components_range, cumulative_variance, marker='x')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance vs. Number of Components')\nplt.xticks(n_components_range)\nplt.grid()\n\n# example: add horizontal line at 60% explained variance\nexplained_variance_60percent = 0.60\nn_components_60percent = np.argmax(np.array(cumulative_variance) >= explained_variance_60percent)\nplt.axhline(y=explained_variance_60percent, color='green', linestyle='--', label='60% Explained Variance')\nplt.axvline(x=n_components_60percent, color='green', linestyle='--')\n\n# example: add horizontal line at 95% explained variance\nexplained_variance_95percent = 0.95\nn_components_95percent = np.argmax(np.array(cumulative_variance) >= explained_variance_95percent)\nplt.axhline(y=explained_variance_95percent, color='red', linestyle='--', label='95% Explained Variance')\nplt.axvline(x=n_components_95percent, color='red', linestyle='--')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:52.890548Z","iopub.execute_input":"2023-07-29T09:55:52.891085Z","iopub.status.idle":"2023-07-29T09:55:53.758863Z","shell.execute_reply.started":"2023-07-29T09:55:52.891040Z","shell.execute_reply":"2023-07-29T09:55:53.757737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting the number of components is of course a trade-off between reducing dimensionality and retaining as much information as possible. Higher numbers of components might capture more variance but can also lead to overfitting. On the other hand, too few components may result in a significant loss of information.\n\nThe first thing we notice from the above plot is that we do already capture all of the variance using only 20 components instead of 22. By defining a certain amount of variance that must be captured we can reduce the number of components even further. For example, looking at the plot we obtain 17 components for defining a variance threshold of 95% and only 6 components for 60% explained variance.","metadata":{}},{"cell_type":"code","source":"# returns the principal components needed to achieve a certain explained variance\ndef GetPCAFeatures(variance_threshold):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df_encoded)\n    pca = PCA(n_components=None)\n    pca.fit(scaled_data)\n    \n    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n    pca = PCA(n_components=n_components)\n    pca.fit(scaled_data)\n    principal_components = pca.transform(scaled_data)\n\n    # create a dataframe containing the reduced feature set\n    df_pca = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    return df_pca","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:53.760538Z","iopub.execute_input":"2023-07-29T09:55:53.761206Z","iopub.status.idle":"2023-07-29T09:55:53.770882Z","shell.execute_reply.started":"2023-07-29T09:55:53.761138Z","shell.execute_reply":"2023-07-29T09:55:53.769619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For easier comparability and visualization we will work with only 3 principal components which reflects an explained variability of around 39%. Let's see if this is enough to form meaningful clusters.","metadata":{}},{"cell_type":"code","source":"df_pca = GetPCAFeatures(0.39)\ndf_pca.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:53.772323Z","iopub.execute_input":"2023-07-29T09:55:53.772973Z","iopub.status.idle":"2023-07-29T09:55:53.832628Z","shell.execute_reply.started":"2023-07-29T09:55:53.772940Z","shell.execute_reply":"2023-07-29T09:55:53.831488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model Building\n\nDuring this section we want to build three different unsupervised clustering models, namely K-Means, DBSCAN, and Hierarchical Clustering, and compare their results both graphically and using different evaluation metrics. The metrics to be considered are:\n\n* Silhouette\n* Davies-Bouldin Index\n\nBoth are commonly used in the context of clustering. 'Silhouette' quantifies how well-separated the clusters are and measures their separation of data points within clusters. The score ranges from -1 to 1. The other evaluation metric called 'Davies-Bouldin Index' measures the average similarity between each cluster and its most similar cluster, relative to the average dissimilarity between the clusters. The lower the value the better the clusters are defined.\n\nFor hyperparameter tuning we will use grid search. This technique searches through a predefined set of hyperparameter values and creates a grid of all possible value combinations. Then it evaluates each combination using one of the mentioned evaluation metrics above.\n\nThe business question we want to answer using the resulting cluster is the following: Can we devide customers into certain distinguishable clusters according to their income and money spent on our products? Answering this question can certainly help to identify the most receptive customer segment and focus our marketing efforts only on that particular group. We can also think of strategies to better involve customers currently spending less money so that we don't loose them completely.","metadata":{}},{"cell_type":"markdown","source":"## 5.1 K-Means\n\nAs already mentioned, K-Means is an unsupervised algorithm for clustering data points into distinct groups. It is a so-called distance-based algorithm since it tries to minimize the sum of squared distances between data points and their assigned cluster centers (= centroids). This optimization process iteratively updates the cluster assignments and the centroids to find the best solution. Each step aims to minimize the within-cluster variance and maximize the separation between clusters. Upfront, you have to specify the number of desired clusters as a hyperparameter.\n\nThe algorithm typically suffers from the \"curse of dimensionality\" which means that a high-dimensional feature space (= many features) causes the algorithm to not find the best clusters. In high-dimensional spaces the data points become very sparse which means that the distance between the points increases and it becomes much more difficult to find meaningful clusters.\n\nThe fact that K-Means may not be the ideal choice for high dimensions is also reflected in the formula for time complexity as follows: O(t * K * n * d)\n\n* t: number of iterations until clusters are formed\n* K: number of specified clusters\n* n: number of data points\n* d: number of dimensions (= features)\n\nThis means time complexity increases linearly with the number of features and also datapoints which both can be quite high numbers. Typically, the number of clusters is a smaller number and therefore not having a huge impact. The number of iterations needed until convergence on the other hand plays a more important role since it heavily depends on the initial placement of the centroids which can be affected by another hyperparameter.\n\nWith all this information we can conclude that this algorithm could be suitable for our data if we do not take too many features into the model. Therefore we will work with a smaller amount of explained variance in the PCA model to avoid the curse of dimensionality. Besides that, K-Means could also act as kind of a baseline model to compare with more sophisticated models like DBSCAN which we will explore later.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.1 Basic model","metadata":{}},{"cell_type":"code","source":"# get a fresh copy of the PCA dataset\ndf_pca_kmeans = df_pca.copy(True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:53.834093Z","iopub.execute_input":"2023-07-29T09:55:53.835016Z","iopub.status.idle":"2023-07-29T09:55:53.840294Z","shell.execute_reply.started":"2023-07-29T09:55:53.834973Z","shell.execute_reply":"2023-07-29T09:55:53.839180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try out different number of clusters\nnum_clusters_range = range(2,10)\n\n# calculate the inertia (= within-cluster sum of squares) for each cluster\n# this is a measure for how well the algorithm performs\n# the lower the inertia the better the clusters are formed\ninertia = []\nfor num_clusters in num_clusters_range:\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(df_pca_kmeans)\n    inertia.append(kmeans.inertia_)\n\n# plot elbow curve\nplt.figure(figsize=(8, 6))\nplt.plot(num_clusters_range, inertia, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('K-Means Clustering')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:53.841961Z","iopub.execute_input":"2023-07-29T09:55:53.842650Z","iopub.status.idle":"2023-07-29T09:55:54.682492Z","shell.execute_reply.started":"2023-07-29T09:55:53.842608Z","shell.execute_reply":"2023-07-29T09:55:54.681245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot shows the inertia (= within cluster sum-of-squares) for each K (= number of clusters). We can see that the inertia becomes smaller for higher number of clusters. We could say that we should choose the highest possible value for K then. But of course clusters become less meaningful the more there are. Therefore, we have to choose a K big enough where we still get meaningful clusters. A popular technique to do so is called the Elbow Method where we try to find the K in the graph where the decline in inertia becomes significantly smaller. From the plot above we see that it is the case for K = 4.\n\nLet's repeat the K-Means for this number of clusters and store the resulting cluster assignment in our dataset.","metadata":{}},{"cell_type":"code","source":"# Perform K-Means for optimal number of clusters acoording to Elbow Method\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans.fit(df_pca_kmeans)\ndf_pca_kmeans['KMeans_Basic'] = kmeans.labels_\ndf_encoded['KMeans_Basic'] = kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:54.684086Z","iopub.execute_input":"2023-07-29T09:55:54.684823Z","iopub.status.idle":"2023-07-29T09:55:54.738017Z","shell.execute_reply.started":"2023-07-29T09:55:54.684784Z","shell.execute_reply":"2023-07-29T09:55:54.737103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the resulting clusters\nfig=plt.figure(figsize=(12,8))\nax=plt.subplot(111, projection='3d')\nax.scatter(df_pca_kmeans['PC1'], df_pca_kmeans['PC2'], df_pca_kmeans['PC3'], s=40, c=df_pca_kmeans['KMeans_Basic'], marker='o', edgecolor='black')\nax.set_title('Plot of the Clusters for K-Means')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:54.739498Z","iopub.execute_input":"2023-07-29T09:55:54.740089Z","iopub.status.idle":"2023-07-29T09:55:55.045145Z","shell.execute_reply.started":"2023-07-29T09:55:54.740052Z","shell.execute_reply":"2023-07-29T09:55:55.044029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see the different clusters formed in the 3D space represented by the 3 most important principal components. Now we will look at the original feature space and plot the total amount spent vs. the income to see if we can identify distinct groups of customers.","metadata":{}},{"cell_type":"code","source":"# plot total amount spent vs. income\nsns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='KMeans_Basic')\nplt.title('K-Means Clusters based on Income and Spending')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:55.046874Z","iopub.execute_input":"2023-07-29T09:55:55.047582Z","iopub.status.idle":"2023-07-29T09:55:55.650103Z","shell.execute_reply.started":"2023-07-29T09:55:55.047540Z","shell.execute_reply":"2023-07-29T09:55:55.649008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot indicates that there are indeed different groups of customers. There seem to be the one with really high income and larger amounts spent as well as those with medium income and medium amount spent. For lower income and money spent there is some overlap between two different clusters. We will see if this can be improved doing hyperparameter optimization. ","metadata":{}},{"cell_type":"markdown","source":"### 5.1.2 Hyperparameter Optimization\n\nBesides the inital value for K we can set an initial state that impacts how fast the algorithm converges. We will try to find the best pair of parameters using a hyperparameter optimization. As already mentioned, we try out two different evaluation metrics to compare with each other. One is called 'Silhouette' and the other is called 'Davies-Bouldin Index'.","metadata":{}},{"cell_type":"code","source":"# get a fresh copy of the PCA dataset\ndf_pca_kmeans = df_pca.copy(True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:55.651381Z","iopub.execute_input":"2023-07-29T09:55:55.651697Z","iopub.status.idle":"2023-07-29T09:55:55.657486Z","shell.execute_reply.started":"2023-07-29T09:55:55.651669Z","shell.execute_reply":"2023-07-29T09:55:55.656250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def KMeansHyper(scoring, columnName):   \n    \n    # define the parameter grid\n    param_grid = {\n        'n_clusters': range(3,5), # number of clusters\n        'n_init': [10, 20, 30, 40],  # number of times K-Means algorithm runs with different centroid seeds\n        'init': ['k-means++', 'random'],  # different centroid initialization methods\n    }\n\n    # perform grid search\n    kmeans = KMeans()\n    grid_search = GridSearchCV(kmeans, param_grid, scoring=scoring)\n    grid_search.fit(df_pca_kmeans)\n\n    # get best parameters\n    best_k = grid_search.best_params_['n_clusters']\n    best_n_init = grid_search.best_params_['n_init']\n    best_init = grid_search.best_params_['init']\n    best_kmeans = grid_search.best_estimator_\n    print(\"Best K:\", best_k)\n    print(\"Iterations on centroid seeds:\", best_n_init)\n    print(\"Best Initialization Method:\", best_init)\n\n    # fit best model and store cluster results\n    best_kmeans.fit(df_pca_kmeans)\n    df_pca_kmeans[columnName] = best_kmeans.labels_\n    df_encoded[columnName] = best_kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:55.659029Z","iopub.execute_input":"2023-07-29T09:55:55.659368Z","iopub.status.idle":"2023-07-29T09:55:55.676206Z","shell.execute_reply.started":"2023-07-29T09:55:55.659331Z","shell.execute_reply":"2023-07-29T09:55:55.673831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"silhouette_scorer = make_scorer(silhouette_score)\nKMeansHyper(silhouette_scorer, 'KMeans_Hyper_Sil')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:55:55.677934Z","iopub.execute_input":"2023-07-29T09:55:55.678317Z","iopub.status.idle":"2023-07-29T09:56:02.760484Z","shell.execute_reply.started":"2023-07-29T09:55:55.678284Z","shell.execute_reply":"2023-07-29T09:56:02.759600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"davies_bouldin_scorer = make_scorer(davies_bouldin_score, greater_is_better=False)\nKMeansHyper(davies_bouldin_scorer, 'KMeans_Hyper_Bouldin')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:02.761901Z","iopub.execute_input":"2023-07-29T09:56:02.762472Z","iopub.status.idle":"2023-07-29T09:56:08.134276Z","shell.execute_reply.started":"2023-07-29T09:56:02.762437Z","shell.execute_reply":"2023-07-29T09:56:08.133184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both evaluation metrics yield the same best hyperparameters for K-Means. Let's visualize the resulting clusters.","metadata":{}},{"cell_type":"code","source":"# plot the resulting clusters\nfig = plt.figure(figsize=(12, 8))\nax1 = plt.subplot(121, projection='3d')\nax2 = plt.subplot(122, projection='3d')\n\nax1.scatter(df_pca_kmeans['PC1'], df_pca_kmeans['PC2'], df_pca_kmeans['PC3'], s=40, c=df_pca_kmeans['KMeans_Hyper_Sil'], marker='o', edgecolor='black')\nax1.set_title('K-Means hyperparameter optimization (Silhouette)')\n\nax2.scatter(df_pca_kmeans['PC1'], df_pca_kmeans['PC2'], df_pca_kmeans['PC3'], s=40, c=df_pca_kmeans['KMeans_Hyper_Bouldin'], marker='o', edgecolor='black')\nax2.set_title('K-Means hyperparameter optimization (Davies-Bouldin)')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:08.136029Z","iopub.execute_input":"2023-07-29T09:56:08.136713Z","iopub.status.idle":"2023-07-29T09:56:09.313328Z","shell.execute_reply.started":"2023-07-29T09:56:08.136668Z","shell.execute_reply":"2023-07-29T09:56:09.312071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we can clearly see the different clusters formed in the 3D space. Let's have a look at the original feature space.","metadata":{}},{"cell_type":"code","source":"# plot total amount spent vs. income\ndef plot_kmeans_spentincome():\n    fig = plt.figure(figsize=(14, 6))\n    ax1 = plt.subplot(121) \n    ax2 = plt.subplot(122) \n\n    sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='KMeans_Hyper_Sil', ax=ax1)\n    ax1.set_title('K-Means hyperparameter optimization (Silhouette)')\n\n    sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='KMeans_Hyper_Bouldin', ax=ax2)\n    ax2.set_title('K-Means hyperparameter optimization (Davies-Bouldin)')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:09.314570Z","iopub.execute_input":"2023-07-29T09:56:09.314879Z","iopub.status.idle":"2023-07-29T09:56:09.322918Z","shell.execute_reply.started":"2023-07-29T09:56:09.314850Z","shell.execute_reply":"2023-07-29T09:56:09.321653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_kmeans_spentincome()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:09.324461Z","iopub.execute_input":"2023-07-29T09:56:09.324797Z","iopub.status.idle":"2023-07-29T09:56:10.410181Z","shell.execute_reply.started":"2023-07-29T09:56:09.324766Z","shell.execute_reply":"2023-07-29T09:56:10.409052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that both evaluation metrics yield (nearly) the same results if we ignore the different color assignment to the clusters. We can easily recognize the 3 three clusters. In contrast to the intermediate result yielded by the basic model we can now clearly identify the clusters.\n\nLet's see if we can reproduce or even improve the results using DBSCAN instead of K-Means.","metadata":{}},{"cell_type":"markdown","source":"## 5.2 DBSCAN\n\nThe term DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and as the name suggests it is a density-based clustering algorithm. It can be used to discover clusters of data points in a dataset but unlike K-Means, it does not require specifying the number of clusters upfront. Instead, the clusters are defined based on the density of data points in the feature space.\n\nIn contrast to K-Means this algorithm does not suffer as much from the \"curse of dimensionality\" since the optimization method does not purely rely on distanced based calulcations. The time complexity of DBSCAN is put together by several factors, also including the number of data points (n) and number of dimensions (= features) of the data (d). In contrast to K-Means we have no K (= number of clusters) or t (= number of iterations) but rather other parameters called Epsilon and MinPts.\n\nEpsilon is defined as a threshold signaling two points are neighbors if the distance between the two points is below that threshold. MinPts on the other hand represents the minimum number of points needed to be called a dense cluster (using Epsilon to identify neighbors).\n\nThe worst-case time complexity of the DBSCAN algorithm is O(n^2) which can be the case for sparse datasets and smaller values of MinPts since this results in more time-intensive neighborhood searches. The average time complexity of DBSCAN is approximately O(n * log n), primarily due to the use of a so-called spatial index to efficiently find neighboring points. However, in cases of dense datasets with a large MinPts value, the complexity can also reach up to O(n^2).\n\nThis means time complexity for DBSCAN heavely relies on the number of data points and also on the choice of parameters Epsilon and MinPts. Considering this information we can say that this algorithm could probably be a better choice for our data.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.1 Basic model","metadata":{}},{"cell_type":"code","source":"# get a fresh copy of the PCA dataset\ndf_pca_dbscan = df_pca.copy(True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:10.411461Z","iopub.execute_input":"2023-07-29T09:56:10.411791Z","iopub.status.idle":"2023-07-29T09:56:10.417256Z","shell.execute_reply.started":"2023-07-29T09:56:10.411762Z","shell.execute_reply":"2023-07-29T09:56:10.416219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a DBSCAN model with arbitrary values for its parameters\ndbscan = DBSCAN(eps=0.5, min_samples=20)\ncluster_labels = dbscan.fit_predict(df_pca_dbscan)\ndf_pca_dbscan['DBSCAN_Basic'] = cluster_labels\ndf_encoded['DBSCAN_Basic'] = cluster_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:10.418820Z","iopub.execute_input":"2023-07-29T09:56:10.419161Z","iopub.status.idle":"2023-07-29T09:56:10.454148Z","shell.execute_reply.started":"2023-07-29T09:56:10.419130Z","shell.execute_reply":"2023-07-29T09:56:10.452933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the resulting clusters\nfig=plt.figure(figsize=(12,8))\nax=plt.subplot(111, projection='3d')\nax.scatter(df_pca_dbscan['PC1'], df_pca_dbscan['PC2'], df_pca_dbscan['PC3'], s=40, c=df_pca_dbscan['DBSCAN_Basic'], marker='o', edgecolor='black')\nax.set_title('Plot of the Clusters for DBSCAN')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:10.455832Z","iopub.execute_input":"2023-07-29T09:56:10.456223Z","iopub.status.idle":"2023-07-29T09:56:10.833410Z","shell.execute_reply.started":"2023-07-29T09:56:10.456160Z","shell.execute_reply":"2023-07-29T09:56:10.832236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='DBSCAN_Basic')\nplt.title('DBSCAN Clusters based on Income and Spending')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:10.834647Z","iopub.execute_input":"2023-07-29T09:56:10.834980Z","iopub.status.idle":"2023-07-29T09:56:11.593307Z","shell.execute_reply.started":"2023-07-29T09:56:10.834952Z","shell.execute_reply":"2023-07-29T09:56:11.592370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtain 6 different clusters that do not seem to be very distinct which can be easily seen in both plot above. This makes it quite hard to draw any conclusions at this point. We can try to apply a hyperparameter optimization in the next step to make sure we were not just unlucky with our initial choice of parameters.  ","metadata":{}},{"cell_type":"markdown","source":"### 5.2.2 Hyperparameter Optimization\n\nIn contrast to K-Means we do not have to specify the number if clusters in advance. Instead we will vary the two parameters 'eps' and 'min_samples'. The first one sets a threshold for the maximum distance between two samples for one to be considered as part of the neighborhood. The second one sets the minimum number of samples in a neighborhood for a point to be considered as a core point. Again we will experiment with the two scorers 'Silhouette' and 'Davies-Bouldin Index'.","metadata":{}},{"cell_type":"code","source":"# get a fresh copy of the PCA dataset\ndf_pca_dbscan = df_pca.copy(True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:11.594739Z","iopub.execute_input":"2023-07-29T09:56:11.595909Z","iopub.status.idle":"2023-07-29T09:56:11.601792Z","shell.execute_reply.started":"2023-07-29T09:56:11.595867Z","shell.execute_reply":"2023-07-29T09:56:11.600244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DBSCANHyper(scoring, columnName):   \n    \n    # define the parameter grid\n    param_grid = {\n        'eps': [1, 10, 1],    # maximum distance between two samples for one to be considered as part of the neighborhood of the other\n        'min_samples': [5, 10, 15, 20],          # minimum number of samples in a neighborhood for a point to be considered as a core point\n    }\n\n    # perform grid search\n    dbscan = DBSCAN()\n    grid_search = GridSearchCV(dbscan, param_grid, scoring=scoring)\n    grid_search.fit(df_pca_dbscan)\n\n    # get best parameters\n    best_dbscan = grid_search.best_estimator_\n    print(\"Best eps:\", grid_search.best_params_['eps'])\n    print(\"Best min_samples:\", grid_search.best_params_['min_samples'])\n\n    # fit best model and store cluster results\n    best_dbscan.fit(df_pca_dbscan)\n    df_pca_dbscan[columnName] = best_dbscan.labels_\n    df_encoded[columnName] = best_dbscan.labels_","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:11.603388Z","iopub.execute_input":"2023-07-29T09:56:11.603877Z","iopub.status.idle":"2023-07-29T09:56:11.620516Z","shell.execute_reply.started":"2023-07-29T09:56:11.603817Z","shell.execute_reply":"2023-07-29T09:56:11.619105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"silhouette_scorer = make_scorer(silhouette_score)\nDBSCANHyper(silhouette_scorer, 'DBSCAN_Hyper_Sil')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:11.622018Z","iopub.execute_input":"2023-07-29T09:56:11.622365Z","iopub.status.idle":"2023-07-29T09:56:13.333183Z","shell.execute_reply.started":"2023-07-29T09:56:11.622334Z","shell.execute_reply":"2023-07-29T09:56:13.331952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"davies_bouldin_scorer = make_scorer(davies_bouldin_score, greater_is_better=False)\nDBSCANHyper(davies_bouldin_scorer, 'DBSCAN_Hyper_Bouldin')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:13.334932Z","iopub.execute_input":"2023-07-29T09:56:13.335341Z","iopub.status.idle":"2023-07-29T09:56:15.146569Z","shell.execute_reply.started":"2023-07-29T09:56:13.335307Z","shell.execute_reply":"2023-07-29T09:56:15.145397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, both evaluation metrics yield the same best hyperparameters. Let's visualize the resulting clusters.","metadata":{}},{"cell_type":"code","source":"# plot the resulting clusters\nfig = plt.figure(figsize=(12, 8))\nax1 = plt.subplot(121, projection='3d')\nax2 = plt.subplot(122, projection='3d')\n\nax1.scatter(df_pca_dbscan['PC1'], df_pca_dbscan['PC2'], df_pca_dbscan['PC3'], s=40, c=df_pca_dbscan['DBSCAN_Hyper_Sil'], marker='o', edgecolor='black')\nax1.set_title('DBSCAN hyperparameter optimization (Silhouette)')\n\nax2.scatter(df_pca_dbscan['PC1'], df_pca_dbscan['PC2'], df_pca_dbscan['PC3'], s=40, c=df_pca_dbscan['DBSCAN_Hyper_Bouldin'], marker='o', edgecolor='black')\nax2.set_title('DBSCAN hyperparameter optimization (Davies-Bouldin)')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:15.148153Z","iopub.execute_input":"2023-07-29T09:56:15.148875Z","iopub.status.idle":"2023-07-29T09:56:15.805547Z","shell.execute_reply.started":"2023-07-29T09:56:15.148830Z","shell.execute_reply":"2023-07-29T09:56:15.804483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot total amount spent vs. income\ndef plot_dbscan_spentincome():\n    fig = plt.figure(figsize=(14, 6))\n    ax1 = plt.subplot(121) \n    ax2 = plt.subplot(122) \n\n    sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='DBSCAN_Hyper_Sil', ax=ax1)\n    ax1.set_title('DBSCAN hyperparameter optimization (Silhouette)')\n\n    sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='DBSCAN_Hyper_Bouldin', ax=ax2)\n    ax2.set_title('DBSCAN hyperparameter optimization (Davies-Bouldin)')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:15.807003Z","iopub.execute_input":"2023-07-29T09:56:15.807361Z","iopub.status.idle":"2023-07-29T09:56:15.815275Z","shell.execute_reply.started":"2023-07-29T09:56:15.807329Z","shell.execute_reply":"2023-07-29T09:56:15.813981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dbscan_spentincome()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:15.816630Z","iopub.execute_input":"2023-07-29T09:56:15.816936Z","iopub.status.idle":"2023-07-29T09:56:16.842306Z","shell.execute_reply.started":"2023-07-29T09:56:15.816900Z","shell.execute_reply":"2023-07-29T09:56:16.841241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This time we do not obtain any meaningful results. Our model identified two clusters but one of them contains nearly all data points and the other one only a few outliers. This could have several reasons which we will discuss in more detail in the Results and Evaluation chapter.\n\nLet's see if we can achieve better results again using a hierarchical clustering approach.","metadata":{}},{"cell_type":"markdown","source":"## 5.3 Hierarchical Clustering\n\nAgglomerative Clustering is a bottom-up hierarchical clustering algorithm that initially begins with each data point as its own cluster. Then it iteratively merges the closest clusters based on a distance metric until a termination criterion is met. The time complexity is O(n^3), with n as the number of data points. The repeated computation of pairwise distances between clusters during each iteration is the reason for this complexity and makes it much more computationally expensive than K-Means and DBSCAN. However, for our case this should not be a problem because we are only working with roughly over 2000 data points.\n\nSince this algorithm can handle different cluster shapes and sizes it seems appropriate for most datasets and also the one we are using. Our data might not be hierarchical by nature but this algorithm could help identify nested clusters.","metadata":{}},{"cell_type":"markdown","source":"### 5.3.1 Basic model\n\nSimilar to K-Means we have to specify the value for the number of clusters upfornt. In order to evaluate resulting clusters we choose the Calinski-Harabasz Index as we did with the inertia for K-Means. Also known as the Variance Ratio Criterion (VRC), this metric measures the ratio of between-cluster variance to within-cluster variance and therefore provides a measure of cluster separation. The higher the index value, the better the clustering solution. In other words we are looking for a peak in the plot instead of using the Elbow Method.","metadata":{}},{"cell_type":"code","source":"# get a fresh copy of the PCA dataset\ndf_pca_agglo = df_pca.copy(True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:16.843860Z","iopub.execute_input":"2023-07-29T09:56:16.844972Z","iopub.status.idle":"2023-07-29T09:56:16.849999Z","shell.execute_reply.started":"2023-07-29T09:56:16.844924Z","shell.execute_reply":"2023-07-29T09:56:16.849004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try out different number of clusters\nnum_clusters_range = range(3,10)\n\n# calculate the calinski_harabasz_score as a measure of how well the clusters are formed\ncalinski_scores = []\nfor num_clusters in num_clusters_range:\n    agglo = AgglomerativeClustering(n_clusters=num_clusters)\n    cluster_labels = agglo.fit_predict(df_pca_agglo)\n    calinski_scores.append(calinski_harabasz_score(df_pca_agglo, cluster_labels))\n\n# plot the index curve\nplt.figure(figsize=(8, 6))\nplt.plot(num_clusters_range, calinski_scores, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Calinski-Harabasz Index')\nplt.title('Agglomerative Clustering')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:16.851332Z","iopub.execute_input":"2023-07-29T09:56:16.852368Z","iopub.status.idle":"2023-07-29T09:56:18.156612Z","shell.execute_reply.started":"2023-07-29T09:56:16.852313Z","shell.execute_reply":"2023-07-29T09:56:18.155183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We identify the peak to be at 6 clusters. This seems to be quite a lot. Let's visualize the result to see more.","metadata":{}},{"cell_type":"code","source":"# create a Agglomerative clustering model with arbitrary values for its parameters\nagglo = AgglomerativeClustering(n_clusters=6)\nagglo_labels = agglo.fit_predict(df_pca_agglo)\ndf_pca_agglo['Agglo_Basic'] = agglo_labels\ndf_encoded['Agglo_Basic'] = agglo_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:18.158161Z","iopub.execute_input":"2023-07-29T09:56:18.158665Z","iopub.status.idle":"2023-07-29T09:56:18.306793Z","shell.execute_reply.started":"2023-07-29T09:56:18.158618Z","shell.execute_reply":"2023-07-29T09:56:18.305653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the resulting clusters\nfig=plt.figure(figsize=(12,8))\nax=plt.subplot(111, projection='3d')\nax.scatter(df_pca_agglo['PC1'], df_pca_agglo['PC2'], df_pca_agglo['PC3'], s=40, c=df_pca_agglo['Agglo_Basic'], marker='o', edgecolor='black')\nax.set_title('Plot of the Clusters for Agglomerative Clustering')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:18.308059Z","iopub.execute_input":"2023-07-29T09:56:18.308477Z","iopub.status.idle":"2023-07-29T09:56:18.608931Z","shell.execute_reply.started":"2023-07-29T09:56:18.308444Z","shell.execute_reply":"2023-07-29T09:56:18.607635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='Agglo_Basic')\nplt.title('Agglomerative Clusters based on Income and Spending')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:18.610542Z","iopub.execute_input":"2023-07-29T09:56:18.610862Z","iopub.status.idle":"2023-07-29T09:56:19.291104Z","shell.execute_reply.started":"2023-07-29T09:56:18.610833Z","shell.execute_reply":"2023-07-29T09:56:19.290033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Despite having quite clearly seperated clusters in the first plot we do not obtain a meaningful clustering for our specific Income vs. Spending plot. Maybe we can improve this by changing the hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"### 5.3.2 Hyperparameter Optimization\n\nEven though we have already found a good value for K, we let the hyperparameter search find the best value in combination with other parameters. Those are 'affinity' and 'linkage'. The first one refers to the distance metric used and the second one to the linkage criterion that decides whether two centroids are merged together or not.","metadata":{}},{"cell_type":"code","source":"# get a fresh copy of the PCA dataset\ndf_pca_agglo = df_pca.copy(True)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:19.292586Z","iopub.execute_input":"2023-07-29T09:56:19.293203Z","iopub.status.idle":"2023-07-29T09:56:19.297586Z","shell.execute_reply.started":"2023-07-29T09:56:19.293147Z","shell.execute_reply":"2023-07-29T09:56:19.296408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def AggloHyper(scoring, columnName):   \n    \n    # define the parameter grid\n    param_grid = {\n        'n_clusters': [3, 4, 5, 6],  # number of clusters\n        'affinity': ['euclidean', 'l1', 'l2', 'manhattan'],  # distance metric\n        'linkage': ['ward', 'complete', 'average', 'single']  # linkage criterion\n    }\n\n    # perform grid search\n    agglomerative = AgglomerativeClustering()\n    grid_search = GridSearchCV(agglomerative, param_grid, scoring=scoring)\n    grid_search.fit(df_pca_agglo)\n\n    # get best parameters\n    best_agglomerative = grid_search.best_estimator_\n    print(\"Best n_clusters:\", grid_search.best_params_['n_clusters'])\n    print(\"Best affinity:\", grid_search.best_params_['affinity'])\n    print(\"Best linkage:\", grid_search.best_params_['linkage'])\n\n    # fit best model and store cluster results\n    best_agglomerative.fit(df_pca_agglo)\n    df_pca_agglo[columnName] = best_agglomerative.labels_\n    df_encoded[columnName] = best_agglomerative.labels_","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:19.298687Z","iopub.execute_input":"2023-07-29T09:56:19.299001Z","iopub.status.idle":"2023-07-29T09:56:19.315259Z","shell.execute_reply.started":"2023-07-29T09:56:19.298972Z","shell.execute_reply":"2023-07-29T09:56:19.313892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"silhouette_scorer = make_scorer(silhouette_score)\nAggloHyper(silhouette_scorer, 'Agglo_Hyper_Sil')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:19.317088Z","iopub.execute_input":"2023-07-29T09:56:19.317754Z","iopub.status.idle":"2023-07-29T09:56:38.335739Z","shell.execute_reply.started":"2023-07-29T09:56:19.317700Z","shell.execute_reply":"2023-07-29T09:56:38.334183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"davies_bouldin_scorer = make_scorer(davies_bouldin_score, greater_is_better=False)\nAggloHyper(davies_bouldin_scorer, 'Agglo_Hyper_Bouldin')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:38.337498Z","iopub.execute_input":"2023-07-29T09:56:38.337832Z","iopub.status.idle":"2023-07-29T09:56:57.534402Z","shell.execute_reply.started":"2023-07-29T09:56:38.337804Z","shell.execute_reply":"2023-07-29T09:56:57.532865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, both evaluation metrics yield the same best hyperparameters. Let's visualize the resulting clusters.","metadata":{}},{"cell_type":"code","source":"# plot the resulting clusters\nfig = plt.figure(figsize=(12, 8))\nax1 = plt.subplot(121, projection='3d')\nax2 = plt.subplot(122, projection='3d')\n\nax1.scatter(df_pca_agglo['PC1'], df_pca_agglo['PC2'], df_pca_agglo['PC3'], s=40, c=df_pca_agglo['Agglo_Hyper_Sil'], marker='o', edgecolor='black')\nax1.set_title('Agglomerative hyperparameter optimization (Silhouette)')\n\nax2.scatter(df_pca_agglo['PC1'], df_pca_agglo['PC2'], df_pca_agglo['PC3'], s=40, c=df_pca_agglo['Agglo_Hyper_Bouldin'], marker='o', edgecolor='black')\nax2.set_title('Agglomerative hyperparameter optimization (Davies-Bouldin)')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:57.535807Z","iopub.execute_input":"2023-07-29T09:56:57.536188Z","iopub.status.idle":"2023-07-29T09:56:58.074208Z","shell.execute_reply.started":"2023-07-29T09:56:57.536136Z","shell.execute_reply":"2023-07-29T09:56:58.073254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot total amount spent vs. income\ndef plot_agglo_spentincome():\n    fig = plt.figure(figsize=(14, 6))\n    ax1 = plt.subplot(121) \n    ax2 = plt.subplot(122) \n\n    sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='Agglo_Hyper_Sil', ax=ax1)\n    ax1.set_title('Agglormerative hyperparameter optimization (Silhouette)')\n\n    sns.scatterplot(data=df_encoded, x='Income', y='TotalSpent', hue='Agglo_Hyper_Bouldin', ax=ax2)\n    ax2.set_title('Agglormerative hyperparameter optimization (Davies-Bouldin)')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:58.075476Z","iopub.execute_input":"2023-07-29T09:56:58.076371Z","iopub.status.idle":"2023-07-29T09:56:58.082529Z","shell.execute_reply.started":"2023-07-29T09:56:58.076336Z","shell.execute_reply":"2023-07-29T09:56:58.081730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_agglo_spentincome()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:58.083750Z","iopub.execute_input":"2023-07-29T09:56:58.084260Z","iopub.status.idle":"2023-07-29T09:56:59.141506Z","shell.execute_reply.started":"2023-07-29T09:56:58.084229Z","shell.execute_reply":"2023-07-29T09:56:59.140341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, the hyperparameter search yields a K = 3 instead of 6 which also seemed to be a reasonable choice considering the plot for the Calinski-Harabasz Index at the beginning of this section. Now, the change in K could be affected by the change of the other parameters during the tuning process. Both plots show one cluster that is clearly seperated whereas the other two are more overlapped which makes it a bit hard to perform a final and clear interpretation of the results.","metadata":{}},{"cell_type":"markdown","source":"# 6. Results and Evaluation\n\nWe worked with three different models, namely K-Means, DBSCAN and Hierarchical respectively Agglomerative Clustering. For each of these models we experimented with a basic model where no hyperparameters were tuned to get a feeling of how good this model might work. Then we performed hyperparameter tuning using grid search and also compared two different evaluation metrics (Silhouette and Davies-Bouldin). We already explained at the beginning of the model building section how grid search as an tuning technique works.\n\nFor each of our models we visualized the clusters in 3D space and also had in mind our business related question which asked if we can devide customers into certain distinguishable clusters according to their income and money spent on our products. In this section we want to summarize the results from the model building section and try to answer this question. We also want to derive which model works best for our dataset.","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Visualizations\n\nFor easier comparison we will plot the resulting spent vs. income plots from our hyperparameter tuning parts again.","metadata":{}},{"cell_type":"code","source":"plot_kmeans_spentincome()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:56:59.143033Z","iopub.execute_input":"2023-07-29T09:56:59.143614Z","iopub.status.idle":"2023-07-29T09:57:00.212294Z","shell.execute_reply.started":"2023-07-29T09:56:59.143580Z","shell.execute_reply":"2023-07-29T09:57:00.211246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dbscan_spentincome()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:57:00.213779Z","iopub.execute_input":"2023-07-29T09:57:00.214890Z","iopub.status.idle":"2023-07-29T09:57:01.214663Z","shell.execute_reply.started":"2023-07-29T09:57:00.214825Z","shell.execute_reply":"2023-07-29T09:57:01.213252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_agglo_spentincome()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T09:57:01.216291Z","iopub.execute_input":"2023-07-29T09:57:01.216695Z","iopub.status.idle":"2023-07-29T09:57:02.264609Z","shell.execute_reply.started":"2023-07-29T09:57:01.216660Z","shell.execute_reply":"2023-07-29T09:57:02.263184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Interpretation\n\nLet's look at the evaluation metrics first. For each of our plot pairs we cannot detect any noticable difference when it comes to the clustering result considering the Silhouette and the Davies-Bouldin metric. Therefore we will only focus on the Silouette plots for our further conlusions.\n\nTalking about the quality of the results it is clear that the DBSCAN clustering performed really bad in answering our business question. We cannot derive any meaningful customer groups from the plot since most the data points belong the same cluster. One reason for this bad result could be that the algorithm is considering almost all data points as noise points, resulting in a single cluster containing the majority of the data. In turn, this could be the result of a epsilon value that is too small. But since our hyperparameter search proposed this value as the best it could be that our data is not ideally suited for this type of algorithm. It could be that the data structure is not optimal or that we should improve our preprocessing steps, e.g. by removing more outliers from other features or taking more principal components into account for the model building itself.\n\nAgglomerative clustering performed much better than DBSCAN but not as good a K-Means. The latter allows for an interpretation based on the detected number of clearly seperated clusters. The clusters we can name are:\n\n* Customers with high income and high spending\n* Customers with intermediate income and intermediate spending\n* Customers with low to intermediate income and less spending\n\nWhat is not reflected by the clusters but what we can notice (ignoring outliers) is that the income range in the last cluster is the largest. This means there are indeed people with at least intermediate income that do not spend a lot on our products (right side of the red cluster in the first pair of plots). By improving our models so that we can get those to be reflected by an extra cluster we could precisely target certain marketing strategies to these customers. This might be very lucrative since there is a lot of room for spending more money on our products. Additionally, the remaining part of the group with lower income could be the target of specific marketing strategies that focus on discounts making more products affordable to people with lower income.\n\nExploring the dataset and clustering further we could try to link this information with the original information about how much was spent on certain products. This way we could target the discounts specifically on those products that are actually of interest.","metadata":{}},{"cell_type":"markdown","source":"# 7. Conclusion\n\nAs a final step in this project let's talk about what did work out well and where things could be improved.","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Learnings and Takeaways\n\nGoing through a complete machine learning project highlights the crucial role of preparation steps such as data cleaning and preprocessing before model building. For example dealing with missing values can be really important when the values belong to one of the features of main interest (Income feature in this case). Techniques like one-hot encoding to deal with categorical data often causes the created features to be correlated.Therefore, subsequent preprocessing steps like PCA that exhibit no correlation in the resulting dataset cannot be ignored. It's also important to recognize that not all models are suitable for refering to the same dataset, and careful consideration is needed when preprocessing the data or selecting the best number of principal components after PCA.","metadata":{}},{"cell_type":"markdown","source":"## 7.2 What did not work\n\nAs already stated the DBSCAN did not work very well. The unsatisfactory outcome may have resulted from the algorithm classifying nearly all data points as noise. This might be a consequence of using a small epsilon value during the hyperparameter search, suggesting that the algorithm may not be an ideal fit. It is possible that the data's structure is suboptimal, or our preprocessing step requires improvement. For instance, addressing outliers in other features or considering additional principal components during model building might yield better results.\n\nAdditionally, it would have been nice to have a fourth cluster showing the customers with intermediate income and lower spending rate. Especially, these people have a huge potential in terms of buying power.","metadata":{}},{"cell_type":"markdown","source":"## 7.3 Possible Improvements\n\nTo improve the clustering results, we should focus on more detailed outlier handling to reduce noise, since noise can negatively impact some clustering algorithms (probably the DBSCAN algorithm). It is also be beneficial to avoid combining too many features to prevent information loss during the clustering process. Besides that, we could experiment with different numbers of Principal Components including a higher explained variance to identify the optimal representation for the data. Of course, trying out other clustering techniques such as Gaussian mixture models can help us find the most suitable algorithm for our specific problem.","metadata":{}}]}